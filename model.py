# -*- coding: utf-8 -*-
"""Untitled18.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19-kNoEcQ1ox6VxD51fnwttrehuJu2jF6
"""

!pip install opencv-python
!pip install numpy
!pip install torch
!pip install transformers
!pip install youtube-transcript-api
!pip install moviepy
!pip install pillow
!pip install pytube
!pip install matplotlib
!pip install scikit-learn
!pip install pandas
!pip install requests
!pip install nltk
!pip install librosa

from urllib.parse import urlparse, parse_qs
import subprocess

import os
import re
import cv2
import numpy as np
import torch
import torch.nn.functional as F
from transformers import AutoTokenizer, AutoModel, pipeline
from youtube_transcript_api import YouTubeTranscriptApi
import moviepy.editor as mp
from moviepy.video.tools.subtitles import SubtitlesClip
from PIL import Image
from pytube import YouTube
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import pandas as pd
from datetime import timedelta
import json
import requests
from google.colab import files
import nltk
from nltk.sentiment import SentimentIntensityAnalyzer
import librosa
from google.colab import drive

# Download necessary NLTK resources
nltk.download('punkt')
nltk.download('vader_lexicon')

# Create directories
os.makedirs('temp', exist_ok=True)
os.makedirs('output', exist_ok=True)
os.makedirs('frames', exist_ok=True)

class VideoAnalyzer:
    def __init__(self):
        # Load sentiment analyzer
        self.sentiment_analyzer = SentimentIntensityAnalyzer()

        # Load text embedding model
        self.text_tokenizer = AutoTokenizer.from_pretrained("sentence-transformers/all-mpnet-base-v2")
        self.text_model = AutoModel.from_pretrained("sentence-transformers/all-mpnet-base-v2")

        # Load emotion detection model
        self.emotion_classifier = pipeline('text-classification',
                                          model='bhadresh-savani/distilbert-base-uncased-emotion',
                                          top_k=None)  # Using top_k=None instead of return_all_scores=True

        # Set up audio model
        self.sample_rate = 22050

        # Initialize platform requirements dictionary
        self.platform_specs = {
            'youtube_shorts': {'max_length': 60, 'aspect_ratio': '9:16', 'caption_style': 'large_centered'},
            'instagram_reels': {'max_length': 90, 'aspect_ratio': '9:16', 'caption_style': 'subtitle'},
            'tiktok': {'max_length': 60, 'aspect_ratio': '9:16', 'caption_style': 'dynamic'}
        }

    def extract_video_id(self, youtube_url):
        """Extract video ID from YouTube URL"""
        parsed_url = urlparse(youtube_url)

        if 'youtube.com' in parsed_url.netloc:
            if 'v' in parse_qs(parsed_url.query):
                return parse_qs(parsed_url.query)['v'][0]
        elif 'youtu.be' in parsed_url.netloc:
            return parsed_url.path[1:]

        return None

    def download_youtube_video(self, url, output_path='temp/original_video.mp4'):
        """Download YouTube video using yt-dlp (more reliable than PyTube)"""
        try:
            video_id = self.extract_video_id(url)
            if not video_id:
                print("Could not extract video ID from URL")
                return None, None

            # Using yt-dlp to download the video
            # Installing yt-dlp if not already installed
            try:
                subprocess.run(["pip", "install", "yt-dlp"], check=True)
            except subprocess.CalledProcessError:
                print("Failed to install yt-dlp")
                return None, None

            # Download the video
            cmd = [
                "yt-dlp",
                "-f", "best[height<=720]",  # Limit resolution to 720p
                "-o", output_path,
                f"https://www.youtube.com/watch?v={video_id}"
            ]

            process = subprocess.run(cmd, check=True, capture_output=True, text=True)

            if os.path.exists(output_path):
                print(f"Successfully downloaded video ID: {video_id}")

                # Get video title from YouTube API
                title_cmd = [
                    "yt-dlp",
                    "--skip-download",
                    "--get-title",
                    f"https://www.youtube.com/watch?v={video_id}"
                ]
                title_process = subprocess.run(title_cmd, check=True, capture_output=True, text=True)
                video_title = title_process.stdout.strip()

                return output_path, video_title
            else:
                print("Download appeared to succeed but file not found")
                return None, None

        except Exception as e:
            print(f"Error downloading video: {e}")
            # Fallback to direct method
            try:
                print("Trying alternate download method...")
                url = f"https://www.youtube.com/watch?v={video_id}"
                cmd = [
                    "python", "-m", "youtube_dl",
                    "-f", "best[height<=720]",
                    "-o", output_path,
                    url
                ]
                subprocess.run(cmd, check=True)

                if os.path.exists(output_path):
                    print("Alternate download successful")
                    return output_path, "Video"
            except Exception as e2:
                print(f"Alternate download also failed: {e2}")

            return None, None

    def extract_transcript(self, youtube_url):
        """Extract transcript from YouTube video"""
        try:
            video_id = self.extract_video_id(youtube_url)
            if not video_id:
                print("Could not extract video ID from URL")
                return None

            transcript = YouTubeTranscriptApi.get_transcript(video_id)
            return transcript
        except Exception as e:
            print(f"Error extracting transcript: {e}")
            return None

    def analyze_transcript_segments(self, transcript):
        """Analyze transcript for interesting segments based on sentiment and emotions"""
        if not transcript:
            return []

        segments = []
        window_size = 5  # Number of transcript entries to combine for analysis

        for i in range(0, len(transcript) - window_size + 1):
            window = transcript[i:i+window_size]
            combined_text = " ".join([entry['text'] for entry in window])

            # Get start and end times
            start_time = window[0]['start']
            end_time = window[-1]['start'] + window[-1]['duration']

            # Analyze sentiment
            sentiment = self.sentiment_analyzer.polarity_scores(combined_text)

            # Analyze emotions
            emotions = self.emotion_classifier(combined_text)

            # Calculate interesting score based on sentiment intensity and emotions
            compound_abs = abs(sentiment['compound'])
            top_emotion = max(emotions[0], key=lambda x: x['score'])
            emotion_score = top_emotion['score']

            interesting_score = (compound_abs * 0.7) + (emotion_score * 0.3)

            segments.append({
                'start': start_time,
                'end': end_time,
                'text': combined_text,
                'sentiment': sentiment,
                'top_emotion': top_emotion['label'],
                'emotion_score': emotion_score,
                'interesting_score': interesting_score
            })

        # Sort segments by interesting score
        segments.sort(key=lambda x: x['interesting_score'], reverse=True)

        return segments

    def extract_audio_features(self, video_path):
        """Extract audio features from video file"""
        # Extract audio from video
        audio_path = 'temp/audio.wav'
        video_clip = mp.VideoFileClip(video_path)
        video_clip.audio.write_audiofile(audio_path, codec='pcm_s16le')

        # Load audio and extract features
        y, sr = librosa.load(audio_path, sr=self.sample_rate)

        # Get audio energy (volume) over time
        hop_length = 512
        frame_length = 2048
        rms = librosa.feature.rms(y=y, frame_length=frame_length, hop_length=hop_length)[0]
        times = librosa.times_like(rms, sr=sr, hop_length=hop_length)

        # Detect beats
        tempo, beat_frames = librosa.beat.beat_track(y=y, sr=sr)
        beat_times = librosa.frames_to_time(beat_frames, sr=sr)

        # Get audio peaks (potential exciting moments)
        peaks = librosa.util.peak_pick(rms, pre_max=10, post_max=10, pre_avg=10, post_avg=10, delta=0.2, wait=10)
        peak_times = times[peaks]

        return {
            'rms': rms,
            'times': times,
            'beat_times': beat_times,
            'peak_times': peak_times
        }

    def analyze_video_frames(self, video_path, sample_rate=1):
        """Analyze video frames for visual interest"""
        cap = cv2.VideoCapture(video_path)
        frame_count = 0
        frame_scores = []
        saved_frames = []
        frame_rate = cap.get(cv2.CAP_PROP_FPS)

        while cap.isOpened():
            ret, frame = cap.read()
            if not ret:
                break

            # Only process every nth frame based on sample_rate
            if frame_count % (frame_rate * sample_rate) == 0:
                # Calculate visual interest score
                # 1. Color variance (more variance often means more visually interesting)
                hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)
                color_std = np.std(hsv[:,:,0])

                # 2. Brightness
                brightness = np.mean(hsv[:,:,2])

                # 3. Edge detection (more edges often means more information/action)
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                edges = cv2.Canny(gray, 100, 200)
                edge_score = np.count_nonzero(edges) / (edges.shape[0] * edges.shape[1])

                # Combined visual interest score
                visual_score = (color_std * 0.4) + (brightness/255 * 0.2) + (edge_score * 0.4)

                # Save frame data
                timestamp = frame_count / frame_rate
                frame_path = f'frames/frame_{frame_count}.jpg'
                cv2.imwrite(frame_path, frame)

                frame_scores.append({
                    'timestamp': timestamp,
                    'visual_score': visual_score,
                    'frame_path': frame_path
                })
                saved_frames.append(frame_path)

            frame_count += 1

        cap.release()

        # Sort frames by visual interest
        frame_scores.sort(key=lambda x: x['visual_score'], reverse=True)

        return frame_scores, saved_frames

    def identify_clip_boundaries(self, transcript_segments, audio_features, frame_scores, min_duration=10, max_duration=60):
        """Identify optimal clip boundaries based on transcript, audio, and visual features"""
        potential_clips = []

        # If no transcript segments were found, create segments based on audio and visual features
        if not transcript_segments:
            print("No transcript segments available. Creating segments based on audio and visual features.")
            # Use audio peaks as starting points for segments
            peak_times = audio_features['peak_times']

            # Ensure we have at least some peak times
            if len(peak_times) < 2:
                # If no peaks, create segments based on regular intervals
                video_duration = max([frame['timestamp'] for frame in frame_scores]) if frame_scores else 60
                num_segments = max(1, int(video_duration / 20))  # Create a segment every 20 seconds
                peak_times = np.linspace(0, video_duration, num_segments)

            # Create segments from peaks
            for i, peak in enumerate(peak_times[:-1]):
                segment_start = peak
                segment_end = min(peak_times[i+1], segment_start + max_duration)

                # If segment is too short, extend it
                if segment_end - segment_start < min_duration:
                    segment_end = min(segment_start + min_duration, max([frame['timestamp'] for frame in frame_scores]))

                # Find visually interesting frames within the segment
                segment_frames = [f for f in frame_scores if segment_start <= f['timestamp'] <= segment_end]
                top_visual_frame = max(segment_frames, key=lambda x: x['visual_score']) if segment_frames else None

                potential_clips.append({
                    'start': segment_start,
                    'end': segment_end,
                    'duration': segment_end - segment_start,
                    'text': "No transcript available",
                    'sentiment': {'compound': 0},
                    'top_emotion': 'neutral',
                    'interesting_score': 0.5,  # Default score
                    'top_visual_frame': top_visual_frame['frame_path'] if top_visual_frame else None,
                    'visual_score': top_visual_frame['visual_score'] if top_visual_frame else 0
                })
        else:
            # Get top transcript segments
            top_segments = transcript_segments[:20]  # Take top 20 interesting segments

            for segment in top_segments:
                segment_start = segment['start']
                segment_end = segment['end']

                # Find nearest audio peaks before and after segment
                nearest_peak_before = max([p for p in audio_features['peak_times'] if p <= segment_start], default=segment_start)
                nearest_peak_after = min([p for p in audio_features['peak_times'] if p >= segment_end], default=segment_end)

                # Expand boundaries to include audio peaks if within reasonable distance
                if segment_start - nearest_peak_before < 5:  # Within 5 seconds
                    adjusted_start = nearest_peak_before
                else:
                    adjusted_start = segment_start

                if nearest_peak_after - segment_end < 5:  # Within 5 seconds
                    adjusted_end = nearest_peak_after
                else:
                    adjusted_end = segment_end

                # Make sure clip is not too short or too long
                duration = adjusted_end - adjusted_start
                if duration < min_duration:
                    # Extend clip to minimum duration
                    extension = (min_duration - duration) / 2
                    adjusted_start = max(0, adjusted_start - extension)
                    adjusted_end = adjusted_end + extension
                elif duration > max_duration:
                    # Trim clip to maximum duration
                    middle = (adjusted_start + adjusted_end) / 2
                    adjusted_start = middle - (max_duration / 2)
                    adjusted_end = middle + (max_duration / 2)

                # Find visually interesting frames within the clip
                clip_frames = [f for f in frame_scores if adjusted_start <= f['timestamp'] <= adjusted_end]
                top_visual_frame = max(clip_frames, key=lambda x: x['visual_score']) if clip_frames else None

                potential_clips.append({
                    'start': adjusted_start,
                    'end': adjusted_end,
                    'duration': adjusted_end - adjusted_start,
                    'text': segment['text'],
                    'sentiment': segment['sentiment'],
                    'top_emotion': segment['top_emotion'],
                    'interesting_score': segment['interesting_score'],
                    'top_visual_frame': top_visual_frame['frame_path'] if top_visual_frame else None,
                    'visual_score': top_visual_frame['visual_score'] if top_visual_frame else 0
                })

        # Remove overlapping clips (prefer higher interesting_score)
        potential_clips.sort(key=lambda x: x['interesting_score'] + x['visual_score'], reverse=True)
        final_clips = []

        for clip in potential_clips:
            # Check if this clip overlaps with any already selected clip
            overlap = False
            for selected in final_clips:
                if (clip['start'] < selected['end'] and clip['end'] > selected['start']):
                    overlap = True
                    break

            if not overlap:
                final_clips.append(clip)

            # Stop once we have enough non-overlapping clips
            if len(final_clips) >= 10:
                break

        for clip in final_clips:
        # Format start and end times as HH:MM:SS
          clip['start_formatted'] = str(timedelta(seconds=int(clip['start'])))
          clip['end_formatted'] = str(timedelta(seconds=int(clip['end'])))

        # Add frame-precise timestamps (if available)
          fps = 30  # Assuming 30fps, adjust if needed
          clip['start_frame'] = int(clip['start'] * fps)
          clip['end_frame'] = int(clip['end'] * fps)

        return final_clips

    def generate_captions(self, video_path, clip_data, style='large_centered'):
        """Generate stylized captions for the video clip"""
        transcript = clip_data['text']

        if style == 'large_centered':
            generator = lambda txt: mp.TextClip(
                txt, font='Arial-Bold', fontsize=50, color='white', stroke_color='black',
                stroke_width=2, method='caption', align='center', size=(720, None)
            ).set_position(('center', 'center'))

        elif style == 'subtitle':
            generator = lambda txt: mp.TextClip(
                txt, font='Arial', fontsize=30, color='white', stroke_color='black',
                stroke_width=1.5, method='caption', align='center', size=(720, None)
            ).set_position(('center', 0.85), relative=True)

        elif style == 'dynamic':
            # For dynamic style, we'll make shorter words larger
            words = transcript.split()
            if len(words) <= 3:
                fontsize = 65
            elif len(words) <= 6:
                fontsize = 50
            else:
                fontsize = 35

            generator = lambda txt: mp.TextClip(
                txt, font='Arial-Bold', fontsize=fontsize, color='white', stroke_color='black',
                stroke_width=2, method='caption', align='center', size=(720, None)
            ).set_position(('center', 'center'))

        return generator(transcript)

    def create_platform_specific_clip(self, video_path, clip_data, platform='youtube_shorts'):
        """Create a clip optimized for a specific platform"""
        specs = self.platform_specs[platform]

        # Load video
        video = mp.VideoFileClip(video_path)

        # Extract clip
        start_time = max(0, clip_data['start'])
        end_time = min(clip_data['end'], start_time + specs['max_length'], video.duration)
        clip = video.subclip(start_time, end_time)

        # Apply aspect ratio
        if specs['aspect_ratio'] == '9:16':
            # Crop to vertical format
            clip_width = clip.w
            clip_height = clip.h
            target_width = clip_height * 9 / 16

            if target_width <= clip_width:
                # Crop width
                x_center = clip_width / 2
                clip = clip.crop(x1=x_center - target_width/2, y1=0,
                                 x2=x_center + target_width/2, y2=clip_height)
            else:
                # Need to add black bars
                clip = clip.resize(height=clip_height, width=target_width)

        # Add captions
        caption = self.generate_captions(video_path, clip_data, style=specs['caption_style'])

        # Set caption duration to match the clip duration
        caption = caption.set_duration(clip.duration)

        # Composite the clip with captions
        final_clip = mp.CompositeVideoClip([clip, caption])

        # Add intro and outro
        intro_text = mp.TextClip(
            f"Top moment - {clip_data['top_emotion'].upper()}",
            fontsize=40, color='white', bg_color='rgba(0,0,0,0.5)',
            size=(clip.w, None), method='caption', align='center'
        ).set_duration(2.0)

        intro_text = intro_text.set_position(('center', 'top'))

        if clip.duration > 2.0:
            final_clip = mp.concatenate_videoclips([
                mp.CompositeVideoClip([clip.subclip(0, 2), intro_text]),
                final_clip.subclip(2)
            ])

        # Add platform-specific branding or effects
        if platform == 'youtube_shorts':
            logo_text = mp.TextClip(
                "#Shorts", fontsize=30, color='white', font='Arial-Bold',
                bg_color='rgba(255,0,0,0.6)', padding_x=10, method='caption'
            ).set_duration(final_clip.duration)
            logo_text = logo_text.set_position((0.05, 0.05), relative=True)
            final_clip = mp.CompositeVideoClip([final_clip, logo_text])

        elif platform == 'instagram_reels':
            logo_text = mp.TextClip(
                "Reels", fontsize=30, color='white', font='Arial-Bold',
                bg_color='rgba(225,48,108,0.6)', padding_x=10, method='caption'
            ).set_duration(final_clip.duration)
            logo_text = logo_text.set_position((0.05, 0.05), relative=True)
            final_clip = mp.CompositeVideoClip([final_clip, logo_text])

        elif platform == 'tiktok':
            logo_text = mp.TextClip(
                "TikTok", fontsize=30, color='white', font='Arial-Bold',
                bg_color='rgba(0,0,0,0.6)', padding_x=10, method='caption'
            ).set_duration(final_clip.duration)
            logo_text = logo_text.set_position((0.05, 0.05), relative=True)
            final_clip = mp.CompositeVideoClip([final_clip, logo_text])

        return final_clip

    def optimize_for_virality(self, clips):
        """Apply additional virality optimization based on content type"""
        optimized_clips = []

        for clip in clips:
            # Determine content type based on emotion and sentiment
            emotion = clip['top_emotion']
            sentiment = clip['sentiment']['compound']

            # Create a copy to avoid modifying the original
            optimized_clip = clip.copy()

            # Apply optimization based on content type
            if emotion in ['joy', 'surprise'] and sentiment > 0.3:
                # Upbeat, positive content
                optimized_clip['virality_score'] = clip['interesting_score'] * 1.3
                optimized_clip['optimal_platform'] = 'youtube_shorts'
                optimized_clip['recommended_tags'] = ['funny', 'entertainment', 'joy', 'feel-good']

            elif emotion in ['anger', 'disgust'] and sentiment < -0.3:
                # Controversial or reaction content
                optimized_clip['virality_score'] = clip['interesting_score'] * 1.2
                optimized_clip['optimal_platform'] = 'tiktok'
                optimized_clip['recommended_tags'] = ['reaction', 'opinion', 'controversy']

            elif emotion in ['sadness', 'fear']:
                # Emotional or storytelling content
                optimized_clip['virality_score'] = clip['interesting_score'] * 1.1
                optimized_clip['optimal_platform'] = 'instagram_reels'
                optimized_clip['recommended_tags'] = ['emotional', 'story', 'perspective']

            else:
                # Neutral or informative content
                optimized_clip['virality_score'] = clip['interesting_score']
                optimized_clip['optimal_platform'] = 'youtube_shorts'
                optimized_clip['recommended_tags'] = ['information', 'tips', 'learn']

            optimized_clips.append(optimized_clip)

        # Sort by virality score
        optimized_clips.sort(key=lambda x: x['virality_score'], reverse=True)

        return optimized_clips

    def process_video(self, youtube_url, num_clips=5):
        """Process YouTube video and generate multiple optimized clips"""
        # Step 1: Download the video
        video_path, video_title = self.download_youtube_video(youtube_url)
        if not video_path:
            return None

        # Step 2: Extract transcript
        transcript = self.extract_transcript(youtube_url)
        if not transcript:
            print("No transcript available. Using alternative analysis methods.")

        # Step 3: Analyze transcript for interesting segments
        transcript_segments = self.analyze_transcript_segments(transcript) if transcript else []

        # Step 4: Extract audio features
        audio_features = self.extract_audio_features(video_path)

        # Step 5: Analyze video frames
        frame_scores, saved_frames = self.analyze_video_frames(video_path)

        # Step 6: Identify optimal clip boundaries
        potential_clips = self.identify_clip_boundaries(transcript_segments, audio_features, frame_scores)

        # Step 7: Optimize clips for virality
        optimized_clips = self.optimize_for_virality(potential_clips)

        # Step 8: Generate platform-specific clips
        results = []

        for i, clip_data in enumerate(optimized_clips[:num_clips]):
            # Determine best platform
            best_platform = clip_data.get('optimal_platform', 'youtube_shorts')

            # Create clips for each platform
            platforms = ['youtube_shorts', 'instagram_reels', 'tiktok']
            platform_clips = {}

            for platform in platforms:
                output_path = f'output/{i+1}_{platform}_{int(clip_data["start"])}-{int(clip_data["end"])}.mp4'
                try:
                    clip = self.create_platform_specific_clip(video_path, clip_data, platform)
                    clip.write_videofile(output_path, codec='libx264', audio_codec='aac')
                    platform_clips[platform] = output_path
                except Exception as e:
                    print(f"Error creating clip for {platform}: {e}")
                    platform_clips[platform] = None

            # Create clip metadata
            clip_info = {
                'clip_index': i+1,
                'video_title': video_title,
                'start_time': str(timedelta(seconds=int(clip_data['start']))),
                'end_time': str(timedelta(seconds=int(clip_data['end']))),
                'start_seconds': float(clip_data['start']),  # Store raw seconds for sorting
                'end_seconds': float(clip_data['end']),      # Store raw seconds for reference
                'start_formatted': clip_data.get('start_formatted', str(timedelta(seconds=int(clip_data['start'])))),
                'end_formatted': clip_data.get('end_formatted', str(timedelta(seconds=int(clip_data['end'])))),
                'duration': int(clip_data['end'] - clip_data['start']),
                'text_content': clip_data['text'],
                'emotion': clip_data['top_emotion'],
                'sentiment': clip_data['sentiment']['compound'],
                'virality_score': clip_data.get('virality_score', clip_data['interesting_score']),
                'recommended_tags': clip_data.get('recommended_tags', []),
                'best_platform': best_platform,
                'file_paths': platform_clips,
                'thumbnail': clip_data['top_visual_frame']
            }

            results.append(clip_info)

        # Save metadata
        with open('output/clip_metadata.json', 'w') as f:
            json.dump(results, f, indent=4)

        return results

class ShortsDashboard:
    def __init__(self, analyzer):
        self.analyzer = analyzer

    def generate_report(self, clips_metadata):
        """Generate a report with visualizations for the clips"""
        if not clips_metadata:
            return "No clips generated. Please check your video and try again."

        # Create a figure with multiple subplots
        fig, axs = plt.subplots(2, 2, figsize=(15, 12))

        # 1. Clip durations
        durations = [clip['duration'] for clip in clips_metadata]
        clip_names = [f"Clip {clip['clip_index']}" for clip in clips_metadata]

        axs[0, 0].bar(clip_names, durations)
        axs[0, 0].set_title('Clip Durations (seconds)')
        axs[0, 0].set_ylabel('Duration (s)')
        axs[0, 0].grid(True, alpha=0.3)

        # 2. Virality Scores
        virality = [clip['virality_score'] for clip in clips_metadata]

        axs[0, 1].bar(clip_names, virality, color='orange')
        axs[0, 1].set_title('Virality Scores')
        axs[0, 1].set_ylabel('Score')
        axs[0, 1].grid(True, alpha=0.3)

        # 3. Emotions pie chart
        emotions = [clip['emotion'] for clip in clips_metadata]
        emotion_counts = {}
        for emotion in emotions:
            if emotion in emotion_counts:
                emotion_counts[emotion] += 1
            else:
                emotion_counts[emotion] = 1

        axs[1, 0].pie(emotion_counts.values(), labels=emotion_counts.keys(), autopct='%1.1f%%')
        axs[1, 0].set_title('Emotion Distribution')

        # 4. Platform recommendations
        platforms = [clip['best_platform'] for clip in clips_metadata]
        platform_counts = {}
        for platform in platforms:
            if platform in platform_counts:
                platform_counts[platform] += 1
            else:
                platform_counts[platform] = 1

        axs[1, 1].bar(platform_counts.keys(), platform_counts.values(), color='green')
        axs[1, 1].set_title('Recommended Platforms')
        axs[1, 1].set_ylabel('Number of Clips')
        axs[1, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('output/analysis_report.png')

        # Create a summary report
        summary = {
            'total_clips': len(clips_metadata),
            'avg_duration': sum(durations) / len(durations),
            'top_emotion': max(emotion_counts.items(), key=lambda x: x[1])[0] if emotion_counts else "unknown",
            'top_platform': max(platform_counts.items(), key=lambda x: x[1])[0] if platform_counts else "youtube_shorts",
            'highest_virality': max(virality) if virality else 0,
            'clips': clips_metadata
        }

        # Save summary to JSON
        with open('output/summary_report.json', 'w') as f:
            json.dump(summary, f, indent=4)

        return summary

# Modify the run_shorts_generator function
def run_shorts_generator(youtube_url, num_clips=5):
    """Main function to run the shorts generator"""
    analyzer = VideoAnalyzer()
    dashboard = ShortsDashboard(analyzer)

    print(f"Processing video: {youtube_url}")
    print("Step 1/3: Analyzing video content...")
    clips_metadata = analyzer.process_video(youtube_url, num_clips)

    if not clips_metadata:
        print("Error: Could not process video.")
        return

    print(f"Step 2/3: Generated {len(clips_metadata)} clips.")
    print("Step 3/3: Creating analysis report...")
    summary = dashboard.generate_report(clips_metadata)

    # Instead of calling the missing method, just print timestamps directly
    print("\n=== TIMESTAMP SUMMARY ===")
    for clip in clips_metadata:
        print(f"Clip {clip['clip_index']}: {clip['start_time']} to {clip['end_time']} ({clip['duration']}s)")

    # Add timestamp report generation
    # timestamp_df = dashboard.generate_timestamp_report(clips_metadata)

    print("\n=== PROCESSING COMPLETE ===")
    print(f"Total clips created: {summary['total_clips']}")
    print(f"Average duration: {summary['avg_duration']:.1f} seconds")
    print(f"Dominant emotion: {summary['top_emotion']}")
    print(f"Top recommended platform: {summary['top_platform']}")
    print(f"Highest virality score: {summary['highest_virality']:.2f}")

    # Print timestamp summary
    print("\n=== TIMESTAMP SUMMARY ===")
    for clip in clips_metadata:
        print(f"Clip {clip['clip_index']}: {clip['start_time']} to {clip['end_time']} ({clip['duration']}s)")

    print("\nResults saved to the 'output' folder.")
    print("Detailed timestamp report saved as timestamp_report.txt and timestamp_report.csv")

    # Return paths for generated files
    return {
        'clips': [clip['file_paths'] for clip in clips_metadata],
        'report': 'output/analysis_report.png',
        'metadata': 'output/summary_report.json',
        'timestamp_report': 'output/timestamp_report.txt'
    }

# Example usage
if __name__ == "__main__":
    # Mount Google Drive to save output
    drive.mount('/content/drive')

    # Example YouTube URL
    youtube_url = input("Enter YouTube video URL: ")
    num_clips = int(input("How many clips to generate? (default: 5): ") or "5")

    results = run_shorts_generator(youtube_url, num_clips)

    # Copy results to Google Drive
    if results:
        drive_output_path = "/content/drive/MyDrive/shorts_generator_output"
        os.makedirs(drive_output_path, exist_ok=True)

        os.system(f"cp -r output/* {drive_output_path}/")
        print(f"All results copied to Google Drive: {drive_output_path}")



!pip install youtube-transcript-api
!pip install googletrans==4.0.0-rc1
!pip install gtts
!pip install pydub

# Import libraries
from yt_dlp import YoutubeDL
from gtts import gTTS
import os
import json
from IPython.display import HTML
from base64 import b64encode

# Load clip metadata from JSON file
def load_clip_metadata(json_file_path):
    with open(json_file_path, 'r') as f:
        return json.load(f)

# Convert timestamp to seconds
def timestamp_to_seconds(timestamp):
    parts = timestamp.split(':')
    if len(parts) == 3:  # hh:mm:ss
        return int(parts[0]) * 3600 + int(parts[1]) * 60 + float(parts[2])
    elif len(parts) == 2:  # mm:ss
        return int(parts[0]) * 60 + float(parts[1])
    else:
        return float(timestamp)

# Download YouTube video in MP4 format
def download_video(video_url, output_file='original_video.mp4'):
    ydl_opts = {
        'format': 'bestvideo[height<=360]+bestaudio/best[height<=360]',
        'merge_output_format': 'mp4',  # Ensure the merged output is MP4
        'outtmpl': output_file,
    }

    if not os.path.exists(output_file):
        with YoutubeDL(ydl_opts) as ydl:
            ydl.download([video_url])

    return output_file

# Extract a specific clip from a downloaded video
def extract_clip(video_file, start_time, end_time, output_video, output_audio):
    # Use FFmpeg to extract the clip
    os.system(f'ffmpeg -i "{video_file}" -ss {start_time} -to {end_time} -c:v copy -c:a copy "{output_video}" -y')

    # Extract audio from the clip
    os.system(f'ffmpeg -i "{output_video}" -vn -acodec pcm_s16le -ar 44100 -ac 2 "{output_audio}" -y')

    print(f"Clip extracted from {start_time} to {end_time} and saved as: {output_video}")
    print(f"Audio extracted and saved as: {output_audio}")

    return output_video, output_audio

# Translate text (placeholder implementation)
def translate_text(text, target_language='es'):
    language_map = {
        'es': f"Traducción al español: {text}",
        'fr': f"Traduction en français: {text}",
        'de': f"Übersetzung auf Deutsch: {text}",
        'it': f"Traduzione in italiano: {text}",
        'ja': f"日本語訳: {text}",
        'zh': f"中文翻译: {text}"
    }

    return language_map.get(target_language, f"Translation to {target_language}: {text}")

# Generate translated audio from text
def generate_translated_audio(text, output_file, target_language='es'):
    tts = gTTS(text=text, lang=target_language)
    tts.save(output_file)
    print(f"Translated audio saved as: {output_file}")
    return output_file

# Synchronize translated audio with the video clip
def synchronize_audio_with_clip(video_file, translated_audio_file, output_video_file):
    os.system(f'ffmpeg -i "{video_file}" -i "{translated_audio_file}" -c:v copy -map 0:v:0 -map 1:a:0 -shortest "{output_video_file}" -y')
    print(f"Synchronized clip saved as: {output_video_file}")
    return output_video_file

# Process a specific clip from the metadata
def process_clip(video_url, clip_data, target_language='es', output_dir='output_clips'):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)

    clip_index = clip_data['clip_index']
    start_time = clip_data['start_formatted']
    end_time = clip_data['end_formatted']
    text_content = clip_data['text_content']

    print(f"\nProcessing clip {clip_index} from {start_time} to {end_time}")
    print(f"Original text: {text_content}")

    start_seconds = timestamp_to_seconds(start_time)
    end_seconds = timestamp_to_seconds(end_time)

    # Download video if not already downloaded
    video_file = "original_video.mp4"
    if not os.path.exists(video_file):
        video_file = download_video(video_url)

    # Extract the clip
    temp_clip_video = f"{output_dir}/temp_clip_{clip_index}.mp4"
    temp_clip_audio = f"{output_dir}/temp_clip_audio_{clip_index}.wav"
    extract_clip(video_file, start_seconds, end_seconds, temp_clip_video, temp_clip_audio)

    # Translate text
    translated_text = translate_text(text_content, target_language)
    print(f"Translated text: {translated_text}")

    # Generate translated audio
    translated_audio_file = f"{output_dir}/translated_audio_{clip_index}.wav"
    generate_translated_audio(translated_text, translated_audio_file, target_language)

    # Synchronize audio with video
    output_file = os.path.join(output_dir, f"translated_clip_{clip_index}.mp4")
    synchronize_audio_with_clip(temp_clip_video, translated_audio_file, output_file)

    return output_file

# Process multiple clips from metadata
def process_clips_from_metadata(json_file_path, video_url, target_language='es'):
    clips_metadata = load_clip_metadata(json_file_path)
    processed_clips = []

    for clip_data in clips_metadata:
        processed_clip = process_clip(video_url, clip_data, target_language)
        processed_clips.append(processed_clip)

    return processed_clips

# Play a video in Jupyter Notebook (Colab)
def play_video(video_file):
    with open(video_file, "rb") as f:
        video_data = f.read()
    video_url = f"data:video/mp4;base64,{b64encode(video_data).decode()}"
    display(HTML(f'<video width="400" controls><source src="{video_url}" type="video/mp4"></video>'))

# Example usage
if __name__ == "__main__":
    video_url = "https://www.youtube.com/watch?v=bSDprg24pEA"  # Replace with actual URL
    json_file_path = "/content/output/clip_metadata.json"  # Ensure the JSON file is in the correct location
    target_language = 'es'  # Target language (Spanish in this example)

    # Process clips from metadata
    processed_clips = process_clips_from_metadata(json_file_path, video_url, target_language)

    # Display the processed clips
    print("\nProcessed clips:")
    for clip_file in processed_clips:
        print(clip_file)

